-*- org -*-

I'm trying to set things up so that we can build from bootstrap on in parallel
for multiple architectures using a common hostprep.

This may or may not involve using out-of-tree autoconf... depending on how much
work it takes (I'd probably have to gut the script-based bootstrap system...)


* Problems [2/4]:
- [X] /tools symlinks

  Maybe /tools-x86_64-64 style throughout?  Wouldn't be hard to run the CLFS
  patch through sed before applying...

  We'll use /tools-$CLFS_ARCH_STRING and /cross-tools-$CLFS_ARCH_STRING by
  default, but allow overriding at configure-time via precious variables.

  For example, to force the same paths as CLFS:

    PREFIX_CROSS=/cross-tools PREFIX_TOOLS=/tools ./configure

- [X] /ruckus can't be passed RO if EXT4?

  Maybe use EXT4 w/out journal? seems sketchy... can we just disable the
  journal when we go to pass through to the domUs?

  [    1.336144] EXT4-fs (xvdb1): write access unavailable, cannot proceed

  -o noload!  that was easy...

  *NOTE* but what happens if fstype isn't ext4?

- [ ] will need some rw space instead of /ruckus

  Perhaps just make sure / is big enough and have a /ruckus_out directory?
  This would be used instead of /ruckus for bootstrap-stage2 and the final
  build phase.  Might need to be a seperate rw partition to keep it out of our
  fs dumps... (could use list of inodes and stat to figure them out from
  filenames, see description of -e in dump(8)).

  We actually need the original build tree (at least bootstrap-early/config)
  once we boot into the build env... so we may as well start the build off at
  /media/baby-x86_64/ruckus-build and use the baby rootfs.

  How much extra space in our install device are we talking about here?

  x86_64:    6.3G
  x86_64-64: 6.2G
  i686:      5.5G

  Of course, those numbers are only up through bootstrap-stage1...  by the
  time we've done the entire stage2 and final build, it might be waaaaay
  larger.

  Of course, we could make clean in /ruckus-build to reduce by that much...

- [ ] will we be building too much

  Maybe we need to limit the number of archs that are built in parallel?  Or
  tone down the NUMJOBS calculation?  Maybe just wait a minute or two in
  between each one so the peaks and valleys of CPU utilization match up better?

  The test builds that I've actually looked at used about 370-385% (of 8 cores)
  building bootstrap over the course of the entire build, but they did redline
  all 8 cores in some sections (glibc, gcc, kernel, busybox).

  Well, I've now tried building all 3 in parallel... The one time I tried it,
  my i686 build died on gmp with a gcc segfault.  System was being flogged
  pretty soundly, but not so much that I would expect gcc to segfault...

  I kicked it back off and then the x86_64 build died in glibc... another
  compiler segfault.

  And then my box locked up...

  Reduced CPUCOUNT in functions to 4, tried again... and locked up again...

  Reduced CPUCOUNT to 2, tried again...

  x86_64:    7391.78s user 2875.86s system 223% cpu 1:16:40.06 total
  i686:      6548.03s user 2332.95s system 221% cpu 1:06:44.49 total
  x86_64-64: 6557.94s user 2359.25s system 221% cpu 1:06:56.89 total

  So a grand total of 1:16 at 665% of 8 cores.


  --with-numjobs= to override detected value? or just make CPUCOUNT a precious
  variable substituted into config instead of hard-coded in functions?

  watch -n1 'for x in baby{,32,64}; do
    ls -1drt /media/$x/ruckus-build/utils/bootstrap-*/.build/* | tail -n1
  done'


* Then we need a build-aux/build script:
./build-aux/build --arch-list=i686,x86_64,x86_64-64

If we're really gonna do this, should we ditch the "--disable-multilib"
configure flag and just add x86_64-64 as a supported architecture?

build script should:
  - verify sudo w/out password for build user
  - root: lvcreate --name baby-$ARCH --size 10G vg00
  - root: mkfs.ext4 -L baby-$ARCH /dev/vg00/baby-$ARCH
  - root: mount /dev/vg00/baby-$ARCH /media/baby-$ARCH
  - root: chown guest /media/baby-$ARCH
  - root: create /tools-$ARCH /cross-tools-$ARCH symlinks
  - ./configure ...
  - make bootstrap
  - root: make bootstrap-install

  - Once all the selected archs have gotten to this point, we should have a xen
    hypervisor to boot up into...  Unfortunately, we probably have to have a
    person reboot and select Xen and follow some procedure to kick us back
    off...

  - . $PREFIX/ruckus_builder.env

** configure examples
Eventually, all the bootstrap's should also use --enable-stage2-auto

Also, they should be setting CPUCOUNT to actual/no_of_archs... perhaps rounded
up or +1.

*** download & hostprep
INIT_SUBMOD_ARGS="-m MIRRORS" ./autogen.sh \
    --prefix=/mnt/root-true/ruckus-hostprep \
    --with-hostprep-only

could alternatively do a download & hostprep & stage1-xen-only

cd /media/ruckus
NOCONFIGURE=1 ./autogen.sh &&
./init_submodules -m MIRRORS &&
./get_sources.tar &&
echo yay

cd /media/hostprep/ruckus-build

/media/ruckus/configure \
    --prefix=/mnt/root-true/ruckus-hostprep \
    --with-hostprep-only \
    --with-arch=x86_64 --disable-multilib \
    --with-stage1-xen-only

make hostprep

not sure how that would work with current .am files...  bootstrap target might
not do anything...

also, might have to tweak the .ac file to do some but not all of the sanity
checks (i.e., check /tools symlinks but don't require install-dev or
ruckus-dev).

I didn't do things that way originally, because my focus was on building x86_64
multilib immediately...  but if focus is shifting to i686, we may want to do
the xen cross-compilation here instead of as a side effect of doing an x86_64
build or via the i686 xen detour...

Actually, it looks like I've already plumbed this into the .am files... as long
as you also specify --with-stage1-xen-only?  We should make
--with-hostprep-only also set stage1_xen_only=yes and maybe
--disable-multilib...

definitely need to setup a /tools-hostprep if we need to cross-compile xen
though...


*** bootstrap x86_64 multilib
CPUCOUNT=2 /media/ruckus/configure \
    --prefix=/mnt/root-true/ruckus-hostprep \
    --with-install-dev=/dev/mapper/vg00-baby \
    --with-ruckus-dev-snapshot=/dev/mapper/vg00-ruckus

*** bootstrap i686
*NOTE* Disable stage1-xen to make sure we don't accidentally do the Xen detour

CPUCOUNT=2 /media/ruckus/configure \
    --prefix=/mnt/root-true/ruckus-hostprep \
    --with-arch=i686 \
    --without-stage1-xen \
    --with-install-dev=/dev/mapper/vg00-baby32 \
    --with-ruckus-dev-snapshot=/dev/mapper/vg00-ruckus

*** bootstrap x86_64 pure64
*NOTE* Disable stage1-xen to make sure we don't build the Hypervisor twice
       (e.g., if we're running 2 x86_64 builds in parallel they might both
       build Xen during stage1)

CPUCOUNT=2 /media/ruckus/configure \
    --prefix=/mnt/root-true/ruckus-hostprep \
    --with-arch=x86_64 --disable-multilib \
    --without-stage1-xen \
    --with-install-dev=/dev/mapper/vg00-baby64 \
    --with-ruckus-dev-snapshot=/dev/mapper/vg00-ruckus



* Stage 2 Problems [0/3]
- [ ] Should we automatically start all the build machines?

  If the hostprep stage is already built (i.e., build it 1st on it's own before
  we run the build script), then we can just go ahead and start the builder
  domUs... But hostprep will have to be FULLY built, including the Xen
  hypervisor.

- [ ] How do we coordinate the multiple build machines?

  How do we know when they're finished building?  We could autostart the build
  process at bootup and shutdown the virtual machine if it finishes
  successfully...?  Once the machines are stopped, we can access all the
  filesystems from the dom0, so that might not be a bad idea.

- [ ] How are we going to collect logfile from build machines?

  Assuming we add build logging to the build script, we'll want to somehow
  capture the output of the stage2 and final build...  I guess we'll want to
  redirect stdout/stderr to logfiles in /ruckus-build so we can get at them
  later.

  Actually, we could even leave our baby filesystems mounted read-only on the
  dom0 and tail the output from there...  but we'd *really* be begging for
  accidental filesystem corruption (e.g., forgot to remount,ro prior to
  running build script).  We would want to automate the remount,ro,noload in
  the build script to prevent lack-of-sleep from contributing to lost data.

  *NOTE*: This doesn't seem to work quite right... I'm seeing updated files in
  the dom0, but freshly created subdirs aren't getting picked up by dom0's
  kernel until the next full remount (i.e., not just -o remount, full umount &&
  mount)?
